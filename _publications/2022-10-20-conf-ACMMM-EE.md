---
title: "End-to-End Compound Table Understanding with Multi-Modal Modeling"
collection: publications
category: conferences
permalink: /publication/2022-10-20-conf-ACMMM-EE
excerpt: 'Zaisheng Li, Yi Li, Qiao Liang, Pengfei Li, Zhanzhan Cheng*, Yi Niu, Shiliang Pu, Xi Li'
date: 2022-10-20
venue: 'ACM MM'
#slidesurl: 'http://academicpages.github.io/files/slides3.pdf'
paperurl: 'https://dl.acm.org/doi/10.1145/3503161.3547885'
#citation: 'Your Name, You. (2015). &quot;Paper Title Number 3.&quot; <i>Journal 1</i>. 1(3).'
---

Authors:
------
Zaisheng Li, Yi Li, Qiao Liang, Pengfei Li, Zhanzhan Cheng*, Yi Niu, Shiliang Pu, Xi Li

Abstract:
------
Table is a widely used data form in webpages, spreadsheets, or PDFs to organize and present structural data. Although studies on table structure recognition have been successfully used to convert image-based tables into digital structural formats, solving many real problems still relies on further understanding of the table, such as cell relationship extraction. The current datasets related to table understanding are all based on the digit format. To boost research development, we release a new benchmark named ComFinTab with rich annotations that support both table recognition and understanding tasks. Unlike previous datasets containing the basic tables, ComFinTab contains a large ratio of compound tables, which is much more challenging and requires methods using multiple information sources. Based on the dataset, we also propose a uniform, concise task form with the evaluation metric to better evaluate the model's performance on the table understanding task in compound tables. Finally, a framework named CTUNet is proposed to integrate the compromised visual, semantic, and position features with a graph attention network, which can solve the table recognition task and the challenging table understanding task as a whole. Experimental results compared with some previous advanced table understanding methods demonstrate the effectiveness of our proposed model. Code and dataset are available at \urlhttps://github.com/hikopensource/DAVAR-Lab-OCR.

[Download Paper](https://dl.acm.org/doi/10.1145/3503161.3547885)