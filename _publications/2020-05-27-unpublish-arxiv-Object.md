---
title: "Object-QA: Towards High Reliable Object Quality Assessment"
collection: publications
category: manuscripts
permalink: /publication/2020-05-27-unpublish-arxiv-Object
excerpt: 'Jing Lu, Baorui Zou, Zhanzhan Cheng*, Shiliang Pu, Shuigeng Zhou, Yi Niu, Fei Wu'
date: 2020-05-27
venue: 'Arxiv'
#slidesurl: 'http://academicpages.github.io/files/slides3.pdf'
paperurl: 'https://arxiv.org/pdf/2005.13116'
#citation: 'Your Name, You. (2015). &quot;Paper Title Number 3.&quot; <i>Journal 1</i>. 1(3).'
---

Authors:
------
Jing Lu, Baorui Zou, Zhanzhan Cheng*, Shiliang Pu, Shuigeng Zhou, Yi Niu, Fei Wu

Abstract:
------
In object recognition applications, object images usually appear with different quality levels. Practically, it is very important to indicate object image qualities for better application performance, e.g. filtering out low-quality object image frames to maintain robust video object recognition results and speed up inference. However, no previous works are explicitly proposed for addressing the problem. In this paper, we define the problem of object quality assessment for the first time and propose an effective approach named Object-QA to assess high-reliable quality scores for object images. Concretely, Object-QA first employs a well-designed relative quality assessing module that learns the intra-class-level quality scores by referring to the difference between object images and their estimated templates. Then an absolute quality assessing module is designed to generate the final quality scores by aligning the quality score distributions in inter-class. Besides, Object-QA can be implemented with only object-level annotations, and is also easily deployed to a variety of object recognition tasks. To our best knowledge this is the first work to put forward the definition of this problem and conduct quantitative evaluations. Validations on 5 different datasets show that Object-QA can not only assess high-reliable quality scores according with human cognition, but also improve application performance.

[Download Paper](https://arxiv.org/pdf/2005.13116)