---
title: "Technical report for iccv 2021 challenge sslad-track3b: Transformers are better continual learners"
collection: publications
category: report
permalink: /publication/2022-01-13-Report-ICCV-track3b
excerpt: 'Duo Li, Guimei Cao, Yunlu Xu, Zhanzhan Cheng*, Yi Niu'
date: 2022-01-13
venue: 'ICCV 2021 SSLAD workshop'
#slidesurl: 'http://academicpages.github.io/files/slides3.pdf'
paperurl: 'https://arxiv.org/pdf/2201.04924'
#citation: 'Your Name, You. (2015). &quot;Paper Title Number 3.&quot; <i>Journal 1</i>. 1(3).'
---

Authors:
------
Duo Li, Guimei Cao, Yunlu Xu, Zhanzhan Cheng*, Yi Niu

Abstract:
------
In the SSLAD-Track 3B challenge on continual learning, we propose the method of COntinual Learning with Transformer (COLT). We find that transformers suffer less from catastrophic forgetting compared to convolutional neural network. The major principle of our method is to equip the transformer based feature extractor with old knowledge distillation and head expanding strategies to compete catastrophic forgetting. In this report, we first introduce the overall framework of continual learning for object detection. Then, we analyse the key elements' effect on withstanding catastrophic forgetting in our solution. Our method achieves 70.78 mAP on the SSLAD-Track 3B challenge test set.

[Download Paper](https://arxiv.org/pdf/2201.04924)